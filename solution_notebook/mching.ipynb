{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7048914,"sourceType":"datasetVersion","datasetId":4056443},{"sourceId":7065210,"sourceType":"datasetVersion","datasetId":4068043}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transliterate -q","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:28:18.284821Z","iopub.execute_input":"2023-12-12T00:28:18.285950Z","iopub.status.idle":"2023-12-12T00:28:32.061913Z","shell.execute_reply.started":"2023-12-12T00:28:18.285910Z","shell.execute_reply":"2023-12-12T00:28:32.060193Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install nltk -q","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:28:32.069383Z","iopub.execute_input":"2023-12-12T00:28:32.069882Z","iopub.status.idle":"2023-12-12T00:28:44.273557Z","shell.execute_reply.started":"2023-12-12T00:28:32.069805Z","shell.execute_reply":"2023-12-12T00:28:44.272230Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install -U -q sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:28:44.275160Z","iopub.execute_input":"2023-12-12T00:28:44.275565Z","iopub.status.idle":"2023-12-12T00:28:56.811659Z","shell.execute_reply.started":"2023-12-12T00:28:44.275531Z","shell.execute_reply":"2023-12-12T00:28:56.809828Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Matching товаров ООО \"ПРОСЕПТ\"","metadata":{}},{"cell_type":"markdown","source":"## Введение","metadata":{}},{"cell_type":"markdown","source":"**ООО «ПРОСЕПТ»** — российская производственная компания, специализирующаяся\nна выпуске профессиональной химии. В своей работе используют опыт ведущих\nмировых производителей и сырье крупнейших химических концернов. Производство и\nлогистический центр расположены в непосредственной близости от Санкт-Петербурга,\nоткуда продукция компании поставляется во все регионы России.\nСайт: https://prosept.ru/\n\n\n**Введение в задачу**:\n\nЗаказчик производит несколько сотен различных товаров бытовой и промышленной\nхимии, а затем продаёт эти товары через дилеров. Дилеры, в свою очередь,\nзанимаются розничной продажей товаров в крупных сетях магазинов и на онлайн\nплощадках.\n\nДля оценки ситуации, управления ценами и бизнесом в целом, заказчик\nпериодически собирает информацию о том, как дилеры продают их товар. Для этого\nони парсят сайты дилеров, а затем сопоставляют товары и цены.\nЗачастую описание товаров на сайтах дилеров отличаются от того описания, что даёт\nзаказчик. Например, могут добавляться новый слова (“универсальный”,\n“эффективный”), объём (0.6 л -> 600 мл). Поэтому сопоставление товаров дилеров с\nтоварами производителя делается вручную.\nЦель этого проекта - разработка решения, которое отчасти автоматизирует процесс\nсопоставления товаров. Основная идея - предлагать несколько товаров заказчика,\nкоторые с наибольшей вероятностью соответствуют размечаемому товару дилера.\nПредлагается реализовать это решение, как онлайн сервис, открываемый в веб-\nбраузере. Выбор наиболее вероятных подсказок делается методами машинного\nобучения.\n\n**Документация к предоставленным данным**:\n\nЗаказчик предоставил несколько таблиц (дамп БД), содержащих необходимые\nданные:\n\n1 marketing_dealer - список дилеров;\n\n2 marketing_dealerprice - результат работы парсера площадок дилеров:\n\n- product_key - уникальный номер позиции;\n\n- price - цена;\n\n- product_url - адрес страницы, откуда собраны данные;\n\n- product_name - заголовок продаваемого товара;\n\n- date - дата получения информации;\n\n- dealer_id - идентификатор дилера (внешний ключ к marketing_dealer)\n\n\n3 marketing_product - список товаров, которые производит и распространяет\nзаказчик;\n\n- article - артикул товара;\n\n- ean_13 - код товара (см. EAN 13)\n\n- name - название товара;\n\n- cost - стоимость;\n\n- min_recommended_price - рекомендованная минимальная цена;\n\n- recommended_price - рекомендованная цена;\n\n- category_id - категория товара;\n\n- ozon_name - названиет товара на Озоне;\n\n- name_1c - название товара в 1C;\n\n- wb_name - название товара на Wildberries;\n\n- ozon_article - описание для Озон;\n\n- wb_article - артикул для Wildberries;\n\n- ym_article - артикул для Яндекс.Маркета;\n\n4 marketing_productdealerkey - таблица матчинга товаров заказчика и товаров\nдилеров\n\n- key - внешний ключ к marketing_dealerprice\n\n- product_id - внешний ключ к marketing_product\n\n- dealer_id - внешний ключ к marketing_dealer","metadata":{}},{"cell_type":"markdown","source":"## План работ\n","metadata":{}},{"cell_type":"markdown","source":"**До дедлайна 19:00 29 ноября**\n\n1. Команда знакомится с предоставленными данными.\n\n2. Формулируется DS задача, утверждается единые схема валидации решений и метрика качества.\n\n3. Выбирается основной способ решения задачи (модель первого этапа с функционалом финального решения),\n    который будет представлен к дедлайну 29 ноября:\n    - Рассматриваются и валидируются разные способы предобработки входных данных модели.\n    - Рассматриваются и валидируются разные ml движки решения.\n\n\n4. Подготовка модели первого этапа к сдаче на ревью в том виде, в котором ею сможет пользоваться BackEnd департамент команды.\n\n5. Подготовка репозитория решения с jupyter notebook, содержащим основные вехи разработки модели первого этапа.\n\n**После дедлайна DS до единого дедлайна**\n\n6. Генерация новых фичей, улучшение схемы предобработки входных данных.\n\n7. Построение и валидация модели второго этапа (реранжирующий классификатор).\n\n8. Тюнинг реранжирующего классификатора.\n\n9. Предоставление финального решения BackEnd команде, помощь в его инициализации.\n\n**10**. Оформление документации и ожидание результатов хакатона.\n","metadata":{}},{"cell_type":"markdown","source":"## Решение (молель первого этапа)","metadata":{}},{"cell_type":"markdown","source":"### Импорты","metadata":{"execution":{"iopub.status.busy":"2023-11-29T10:29:09.256646Z","iopub.execute_input":"2023-11-29T10:29:09.257404Z","iopub.status.idle":"2023-11-29T10:29:09.263582Z","shell.execute_reply.started":"2023-11-29T10:29:09.257361Z","shell.execute_reply":"2023-11-29T10:29:09.262276Z"}}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import KFold\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom transliterate import translit\nimport re\n\nfrom fuzzywuzzy import fuzz\n\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModel\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nimport os\nimport json\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:28:56.815549Z","iopub.execute_input":"2023-12-12T00:28:56.815971Z","iopub.status.idle":"2023-12-12T00:29:03.200466Z","shell.execute_reply.started":"2023-12-12T00:28:56.815934Z","shell.execute_reply":"2023-12-12T00:29:03.199435Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Загрузка данных","metadata":{}},{"cell_type":"code","source":"!ls -lah","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    dealer = pd.read_csv('/kaggle/input/privat-matching/data/marketing_dealer.csv', on_bad_lines=\"skip\", encoding='utf-8', sep=';')\n    dealerprice = pd.read_csv('/kaggle/input/privat-matching/data/marketing_dealerprice.csv', on_bad_lines=\"skip\", encoding='utf-8', sep=';')\n    product = pd.read_csv('/kaggle/input/privat-matching/data/marketing_product.csv', on_bad_lines=\"skip\", encoding='utf-8', sep=';')\n    interactions = pd.read_csv('/kaggle/input/privat-matching/data/marketing_productdealerkey.csv', on_bad_lines=\"skip\", encoding='utf-8', sep=';')\nexcept:\n    dealer = pd.read_csv('marketing_dealer.csv', on_bad_lines=\"skip\", encoding='utf-8', sep=';')\n    dealerprice = pd.read_csv('marketing_dealerprice.csv', on_bad_lines=\"skip\", encoding='utf-8', sep=';')\n    product = pd.read_csv('marketing_product.csv', on_bad_lines=\"skip\", encoding='utf-8', sep=';')\n    interactions = pd.read_csv('marketing_productdealerkey.csv', on_bad_lines=\"skip\", encoding='utf-8', sep=';')\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:03.201911Z","iopub.execute_input":"2023-12-12T00:29:03.202983Z","iopub.status.idle":"2023-12-12T00:29:03.345170Z","shell.execute_reply.started":"2023-12-12T00:29:03.202943Z","shell.execute_reply":"2023-12-12T00:29:03.344096Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Знакомство с данными","metadata":{}},{"cell_type":"markdown","source":"**Таблица с результатами парсинга**","metadata":{}},{"cell_type":"code","source":"dealerprice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dealerprice.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dealerprice.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Почему-то product_key хранится как строки, хотя должен храниться как числа. Проблема какая-то.\n\n**p.s.** Некоторые диллеры хранят ключи своих товаров как ссылки ","metadata":{}},{"cell_type":"code","source":"dealerprice.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Уникальных диллеров в данных парсера: {len(dealerprice['dealer_id'].unique())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prosept_count = dealerprice['product_name'].apply(lambda x: 1 if x.lower().find('prosept') != -1 else 0).copy().rename('prosept_count')\nprint(f\"Название компании Prosept встречается в {prosept_count.sum()} названиях товаров ({prosept_count.mean() * 100:.2f}%)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Таблица с информацией о продуктах заказчиков**","metadata":{}},{"cell_type":"code","source":"product.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Таблица с сопоставлением продуктов дилеров продуктам заказчиков**","metadata":{}},{"cell_type":"code","source":"interactions.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interactions.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interactions.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interactions.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Предобработка грубых недостатков таблиц","metadata":{}},{"cell_type":"markdown","source":"*ссылки в ключах диллера*\n\n**p.s.**  оказалось что не надо удалять","metadata":{}},{"cell_type":"code","source":"# rows_indexes = dealerprice[dealerprice['product_key'].apply(lambda x: not x.strip().isdigit())].index\n# dealerprice = dealerprice.drop(rows_indexes)\n# del rows_indexes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*ссылки в ключах взаимодействий*\n\n**p.s.**  оказалось что не надо удалять","metadata":{}},{"cell_type":"code","source":"# rows_indexes = interactions[interactions['key'].apply(lambda x: not x.strip().isdigit())].index\n# interactions = interactions.drop(rows_indexes)\n# del rows_indexes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# interactions = interactions.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*пропуски в title PRODUCT* ","metadata":{}},{"cell_type":"code","source":"product[product['name'].isna()]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:03.346619Z","iopub.execute_input":"2023-12-12T00:29:03.347083Z","iopub.status.idle":"2023-12-12T00:29:03.376136Z","shell.execute_reply.started":"2023-12-12T00:29:03.347044Z","shell.execute_reply":"2023-12-12T00:29:03.374908Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    Unnamed: 0   id   article  ean_13 name  cost  recommended_price  \\\n23          23  503  0024-7 о     NaN  NaN   NaN                NaN   \n35          35  504   w022-05     NaN  NaN   NaN                NaN   \n\n    category_id ozon_name name_1c wb_name  ozon_article   wb_article  \\\n23          NaN       NaN     NaN     NaN           NaN  150126213.0   \n35          NaN       NaN     NaN     NaN           NaN          NaN   \n\n   ym_article wb_article_td  \n23        NaN           NaN  \n35        NaN           NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>article</th>\n      <th>ean_13</th>\n      <th>name</th>\n      <th>cost</th>\n      <th>recommended_price</th>\n      <th>category_id</th>\n      <th>ozon_name</th>\n      <th>name_1c</th>\n      <th>wb_name</th>\n      <th>ozon_article</th>\n      <th>wb_article</th>\n      <th>ym_article</th>\n      <th>wb_article_td</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>503</td>\n      <td>0024-7 о</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>150126213.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>35</td>\n      <td>504</td>\n      <td>w022-05</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"product = product.dropna(subset=['name', 'recommended_price'])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:03.377653Z","iopub.execute_input":"2023-12-12T00:29:03.378211Z","iopub.status.idle":"2023-12-12T00:29:03.386891Z","shell.execute_reply.started":"2023-12-12T00:29:03.378175Z","shell.execute_reply":"2023-12-12T00:29:03.385516Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"product = product.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:03.388574Z","iopub.execute_input":"2023-12-12T00:29:03.389079Z","iopub.status.idle":"2023-12-12T00:29:03.399573Z","shell.execute_reply.started":"2023-12-12T00:29:03.389038Z","shell.execute_reply":"2023-12-12T00:29:03.398254Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"*дубли в id диллеров*\n\n**p.s.** Ключи продуктов уникальные лишь в рамках отдельного диллера, так что удаляем дубликаты по двум столбцам. Чтобы в датасете остались только актуальные записи, перед удалением дублей сортируем таблицу по дате.","metadata":{}},{"cell_type":"code","source":"dealerprice = dealerprice.sort_values('date', ascending=False).drop_duplicates(subset=['product_key', 'dealer_id'])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:03.401680Z","iopub.execute_input":"2023-12-12T00:29:03.402150Z","iopub.status.idle":"2023-12-12T00:29:03.433005Z","shell.execute_reply.started":"2023-12-12T00:29:03.402110Z","shell.execute_reply":"2023-12-12T00:29:03.431374Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dealerprice = dealerprice.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:03.434455Z","iopub.execute_input":"2023-12-12T00:29:03.434837Z","iopub.status.idle":"2023-12-12T00:29:03.443489Z","shell.execute_reply.started":"2023-12-12T00:29:03.434805Z","shell.execute_reply":"2023-12-12T00:29:03.441967Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#### Функции чтения и первичной обработки входящих сырых таблиц","metadata":{}},{"cell_type":"markdown","source":"Функция чтения  **marketing_dealerprice**:","metadata":{}},{"cell_type":"code","source":"def dealerprice_table(table_path='marketing_dealerprice.csv',\n                      product_id_column='product_key',\n                      dealer_id_column='dealer_id',\n                      read_params={'on_bad_lines': \"skip\",\n                                   'encoding': 'utf-8',\n                                   'sep': ';'}\n                     ):\n    '''\n    Функция принимает:\n    .Путь к csv файлу, содержащему результаты парсинга.\n    .Названия колонок с id товаров и id дилеров\n    .Параметры чтения csv можно указать, если вдруг они изменятся.\n    '''\n    \n    table_csv = pd.read_csv(table_path, **read_params) \n    table_csv = table_csv.sort_values('date', ascending=False).drop_duplicates(subset=[product_id_column, dealer_id_column])\n    \n    return table_csv\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Функция для чтения **marketing_product**","metadata":{}},{"cell_type":"code","source":"def prossept_products_table(table_path='marketing_product.csv',\n                            product_names_column = 'name',\n                            read_params={'on_bad_lines': \"skip\",\n                                           'encoding': 'utf-8',\n                                           'sep': ';'}\n                             ):\n    '''\n    Функция принимает путь к csv файлу, содержащему актуальную информацию по товарам заказчика.\n    Дополнительно указывается название колонки с внутренними неймингами для удаления плохих строк.\n    '''\n    \n    table_csv = pd.read_csv(table_path, **read_params) \n    table_csv = table_csv.dropna(subset='name')\n    \n    return table_csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Текущие функции подвергнутся изменениям и улучшениям в процессе инициализации решения с бекендом в прод.**","metadata":{}},{"cell_type":"markdown","source":"### Формулирование DS задачи","metadata":{}},{"cell_type":"markdown","source":"#### Роль алгоритма в функционале приложения\n\nНаш алгоритм должен помочь разметчику сопоставить товар диллера с одним из нескольких сотен товаров фирмы заказчика. Важно отметить, что финальное решение принимает именно разметчик. Так вот, насколько видит наша команда, алгоритм для каждого из предложенных товаров диллеров должен вернуть ранжированный список всех товаров заказчика так, чтобы релевантный айтем оказался максимально высоко в топе. Таким образом перед нами тривиальная задача ранжирования.\n\n#### Особенности задачи\n\n- Для товара диллера у заказчика есть только один релевантный айтем.\n- Основными признаками для матчинга выступают нейминги товаров.\n- Товары диллеров всегда новые, а множество товаров заказчика меняется редко.\n\n#### Метрика качества\n\nВ качестве метрики качества мы утвердили Среднеобратный ранг (Mean Reciprocal Rank): $MRR = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\text{rank}_i}$ ,\nгде ${\\text{rank}_i}$ это позиция релевантного айтема заказчика в ранжированном списке а ${N}$ это мощность множества товаров дилеров. Метрика выбрана по ряду причин, и по нашему общему мнению идеально совпадает с нашей задачей. MRR это ничто иное как средняя позиция правильного ответа (совпадения / релевантного item), для удобства шкалированная от 0 до 1. Чтобы из MRR получить среднюю позицию правильного ответа, достаточно делить на неё единицу. В случае одного релевантного товара nDCG (популярная метрика ранжирования) и MRR будут равны с точностью до константы.\n\n#### Схема валидации\n\nТ.к. соответствия товаров диллеров товарам заказчика не изменяются со временем, обычная кросс-валидация идеально нам подходит. Будем валидироваться на 5 фолдах, в тестовой выборке каждого фолда будет примерно 300-400 соответствий.\n\n#### Признаки айтемов \n**Первый этап**:\nКак ранее указывалось, основным признаком для сопоставления являются нейминги товаров. Таким образом в качестве модели первого этапа мы представим функцию, помещающую все нейминги товаров заказчика в единое векторное пространство с помощью текстового векторизатора. Для формирования списка рекомендаций нейминг товара дилера помещается в то же пространство, а затем товары заказчика ранжируются по косинусной близости. В итоге уже обученная модель будет принимать массив с неймингами товаров дилера, а возвращать двумерный массив с рекомендациями.\n\n**Второй этап**:\nВ качестве признаков модель второго этапа (классификатор) для каждой пары `товар_дилера - товар_заказчика` будет принимать вектора неймингов этих товаров, косинусное расстояние между векторами, а так же другие features какие мы нагенерируем. Далее по предсказанным вероятностям соответствия все айтемы заказчика реранжируются.","metadata":{}},{"cell_type":"markdown","source":"## Моделирование","metadata":{}},{"cell_type":"markdown","source":"Заглушка для бека","metadata":{}},{"cell_type":"code","source":"class PopularRecommender():\n\n    def __init__(self, ):\n        pass\n\n    def fit(self,\n            interactions,\n            product_id='product_id'):\n        \n        self.recs = interactions[product_id].value_counts().index.tolist()\n\n    def recommend(self,\n                  dealer_ids: list[dict]):\n        \n        return np.array([self.recs for i in dealer_ids])\n    \nmodel = PopularRecommender()\nmodel.fit(interactions)\nmodel.recommend([1, 2, 3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Модель первого этапа","metadata":{}},{"cell_type":"markdown","source":"Основным движком матчинга будет модель векторизации неймингов. На следующем этапе мы подтянем другие фичи для сопоставления. Например цены товаров. ","metadata":{}},{"cell_type":"markdown","source":"*функция предобработки неймингов для простых векторайзеров:*","metadata":{}},{"cell_type":"code","source":"ru_stop = stopwords.words('russian')\neng_stop = stopwords.words('english')\nru_stemmer = SnowballStemmer(\"russian\")\neng_stemmer = SnowballStemmer(\"english\")\n\ndef string_filter(string,\n                  ru_stop=ru_stop,\n                  eng_stop=eng_stop,\n                  ru_stemmer=ru_stemmer,\n                  eng_stemmer=eng_stemmer):\n    \n    string = string.lower() \n    string = re.sub(r'[^a-zo0-9а-я\\s:]', '', string)\n    string = re.sub(r'(?<=[а-я])(?=[a-z])|(?<=[a-z])(?=[а-я])', ' ', string)\n    string = re.sub(r'(?<=[а-яa-z])(?=\\d)|(?<=\\d)(?=[а-яa-z])', ' ', string)\n    string = ' '.join([eng_stemmer.stem(ru_stemmer.stem(word)) for word in string.split() if word not in ru_stop+eng_stop])\n\n    return string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DistanceRecommender():\n\n    def __init__(self,\n                 vectorizer,\n                 simularity_func,\n                 text_prep_func):\n        \n\n        \n        self.vectorizer = vectorizer\n        self.simularity_counter = simularity_func\n        self.preprocessing = text_prep_func\n\n    def fit(self,\n            product_corpus,\n            name_column,\n            id_column):\n        preprocessed_corpus = product_corpus[name_column].apply(self.preprocessing).values.tolist()\n        \n        self.vectorizer.fit(preprocessed_corpus)\n        self.product_matrix = self.vectorizer.transform(preprocessed_corpus)\n        self.product_index_to_id = {i: product_corpus.loc[i, id_column] for i in range(len(product_corpus))}\n        \n    def recommend(self,\n                  dealer_corpus: list[dict]\n                 ):\n    \n        preprocessed_corpus = dealer_corpus.apply(self.preprocessing).values.tolist()\n        vectors = self.vectorizer.transform(preprocessed_corpus)\n        sims = self.simularity_counter(vectors, self.product_matrix)\n        \n        result = []\n        for vec in sims:\n            result += [[self.product_index_to_id[index] for index in vec.argsort()[::-1]]]\n        return np.array(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Метрика MRR@10, для удобства своя реализация метрики Mean Reciprocal Rank ( https://rectools.readthedocs.io/en/latest/api/rectools.metrics.ranking.MRR.html ). Метрика идеально ложится на нашу задачу. Скалируется от 0 до 1, чем больше метрика тем выше релевантный айтем в списке рекоммендаций. ","metadata":{}},{"cell_type":"code","source":"def accuracy_k(true_ids, recommendations, k):\n    results = 0\n    for index, true_id in enumerate(true_ids):\n        if true_id in recommendations[index][:k]:\n            results += 1\n    return results / len(true_ids)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:33:18.264963Z","iopub.execute_input":"2023-12-12T00:33:18.265381Z","iopub.status.idle":"2023-12-12T00:33:18.272011Z","shell.execute_reply.started":"2023-12-12T00:33:18.265352Z","shell.execute_reply":"2023-12-12T00:33:18.270783Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def mean_reciprocal_rank(true_id,\n                         recommendations,\n                         k=10):\n    \n    reciprocal_ranks = []\n    \n    for i, rec in enumerate(recommendations):\n        recs = rec[:k]\n        relevant = true_id[i]\n        \n        if np.isin(relevant, recs):\n            rank = np.where(recs == relevant)[0][0] + 1\n            reciprocal_ranks += [1 / rank]\n            \n        else:\n            reciprocal_ranks += [0]\n            \n    return np.mean(reciprocal_ranks)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:33:19.844456Z","iopub.execute_input":"2023-12-12T00:33:19.844983Z","iopub.status.idle":"2023-12-12T00:33:19.853499Z","shell.execute_reply.started":"2023-12-12T00:33:19.844949Z","shell.execute_reply":"2023-12-12T00:33:19.852236Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"models = {\n    'CosRecBow_1n': DistanceRecommender(vectorizer=CountVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecBow_1n_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecBow_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecBow_1n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecBow_2n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecBow_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecTfIDF_1n': DistanceRecommender(vectorizer=TfidfVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecTfIDF_1n_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecTfIDF_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecTfIDF_1n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecTfIDF_2n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter),\n    'CosRecTfIDF_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter)\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Все модели первого этапа обучаются на корпусе неймингов заказчика:","metadata":{}},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**кросс-валидация**","metadata":{"execution":{"iopub.status.busy":"2023-11-26T19:54:29.199713Z","iopub.execute_input":"2023-11-26T19:54:29.200198Z","iopub.status.idle":"2023-11-26T19:54:29.209046Z","shell.execute_reply.started":"2023-11-26T19:54:29.200164Z","shell.execute_reply":"2023-11-26T19:54:29.206932Z"}}},{"cell_type":"markdown","source":"Валидировать решения будем на обычной кросс-валидации с 5 фолдами, ту же схему валидации будем использовать и для модели второго этапа. Все результаты будут сохраняться в единый фрейм:","metadata":{}},{"cell_type":"code","source":"dealerprice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table = pd.DataFrame()\n\nfor name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name + '_string_filter'])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.sort_values('MRR', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Лучше всего срабатывает поиск ближайших по косиносному расстоянию на TFIDF с униграммами. Попробуем улучишить функцию предобработки неймингов так, чтобы она приводила литры в миллилитры и килограммы в граммы:","metadata":{}},{"cell_type":"code","source":"def replace_values_l(value):\n    if ' л' in value:\n        value = value.replace(' л', '000 мл')\n        value = value.replace('.0', '')  \n        return value\n    elif 'л' in value:\n        pattern = r'(\\d+(?:\\.\\d+)?)\\s*л\\b'  \n        matches = re.findall(pattern, value, flags=re.IGNORECASE)\n        for match in matches:\n            replacement = f\"{float(match) * 1000:.0f} мл\"  \n            value = re.sub(fr'({match})\\s*л\\b', replacement, value, flags=re.IGNORECASE)\n        return value\n    else:\n        return value\n\ndef replace_values_kg(value):\n    if ' кг' in value:\n        value = value.replace(' кг', '000 г')\n        value = value.replace('.0', '')  \n        return value\n    elif 'кг' in value:\n        pattern = r'(\\d+(?:\\.\\d+)?)\\s*кг\\b' \n        matches = re.findall(pattern, value, flags=re.IGNORECASE)\n        for match in matches:\n            replacement = f\"{float(match) * 1000:.0f} г\"  \n            value = re.sub(fr'({match})\\s*кг\\b', replacement, value, flags=re.IGNORECASE)\n        return value\n    else:\n        return value\n\ndef string_filter_v2(string,\n                  ru_stop=ru_stop,\n                  eng_stop=eng_stop,\n                  ru_stemmer=ru_stemmer,\n                  eng_stemmer=eng_stemmer):\n    \n    string = string.lower() \n    string = replace_values_kg(replace_values_l(string))\n    string = re.sub(r'[^a-zo0-9а-я\\s:]', '', string)\n    string = re.sub(r'(?<=[а-я])(?=[a-z])|(?<=[a-z])(?=[а-я])', ' ', string)\n    string = re.sub(r'(?<=[а-яa-z])(?=\\d)|(?<=\\d)(?=[а-яa-z])', ' ', string)\n    string = ' '.join([eng_stemmer.stem(ru_stemmer.stem(word)) for word in string.split() if word not in ru_stop+eng_stop])\n\n    return string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n    'CosRecBow_1n': DistanceRecommender(vectorizer=CountVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecBow_1n_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecBow_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecBow_1n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecBow_2n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecBow_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecTfIDF_1n': DistanceRecommender(vectorizer=TfidfVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecTfIDF_1n_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecTfIDF_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecTfIDF_1n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecTfIDF_2n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2),\n    'CosRecTfIDF_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2)\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name + 'filter_v2'])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.sort_values('MRR', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Метрика подскочила, это хорошо. Теперь лидирует мешок слов со стандартными весами. Попробуем разбивать слова текста на пары или тройки букв и строить мешки из них. Разумеется, надо увеличивать количество n-грамм в мешках: ","metadata":{}},{"cell_type":"code","source":"def generate_ngrams(text, n):\n    ngrams = [text[i:i + n] for i in range(len(text) - n + 1)]\n    return ngrams\n\ndef string_filter_v3(string,\n                  ru_stop=ru_stop,\n                  eng_stop=eng_stop,\n                  ru_stemmer=ru_stemmer,\n                  eng_stemmer=eng_stemmer,\n                  ngram_len=2):\n    \n    string = string.lower() \n    string = replace_values_kg(replace_values_l(string))\n    string = re.sub(r'[^a-zo0-9а-я\\s:]', '', string)\n    string = re.sub(r'(?<=[а-я])(?=[a-z])|(?<=[a-z])(?=[а-я])', ' ', string)\n    string = re.sub(r'(?<=[а-яa-z])(?=\\d)|(?<=\\d)(?=[а-яa-z])', ' ', string)\n    string = ' '.join([eng_stemmer.stem(ru_stemmer.stem(word)) for word in string.split() if word not in ru_stop+eng_stop])\n    string = ' '.join([' '.join(generate_ngrams(word, ngram_len)) if (len(word) >= ngram_len and not word.isdigit()) else word for word in string.split()])\n    return string\n\nstring = dealerprice['product_name'][0]\nprint(string)\nprint(string_filter_v3(string))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n    'CosRecBow_1n': DistanceRecommender(vectorizer=CountVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecBow_1n_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecBow_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecBow_1n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecBow_2n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecBow_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecBow_1n_4n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecBow_2n_4n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecBow_3n_4n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(3,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecBow_4n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(4,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_1n': DistanceRecommender(vectorizer=TfidfVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_1n_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_1n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_2n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_1n_4n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_2n_4n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_3n_4n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(3,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n    'CosRecTfIDF_4n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(4,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3),\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name + 'filter_v3'])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.sort_values('MRR', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Результаты не улучшились. Попробуем добавить в предобработку транслитизацию. Сначала затестим без разбиения на пары символов:","metadata":{}},{"cell_type":"code","source":"def string_filter_v2_t(string,\n                  ru_stop=ru_stop,\n                  eng_stop=eng_stop,\n                  ru_stemmer=ru_stemmer,\n                  eng_stemmer=eng_stemmer,\n                  transliterator=translit):\n    \n    string = string.lower() \n    string = replace_values_kg(replace_values_l(string))\n    string = re.sub(r'[^a-zo0-9а-я\\s:]', '', string)\n    string = re.sub(r'(?<=[а-я])(?=[a-z])|(?<=[a-z])(?=[а-я])', ' ', string)\n    string = re.sub(r'(?<=[а-яa-z])(?=\\d)|(?<=\\d)(?=[а-яa-z])', ' ', string)\n    string = ' '.join([eng_stemmer.stem(ru_stemmer.stem(word)) for word in string.split() if word not in ru_stop+eng_stop])\n\n    string = transliterator(string, 'ru', reversed=False)\n    return string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n    'CosRecBow_1n': DistanceRecommender(vectorizer=CountVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecBow_1n_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecBow_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecBow_1n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecBow_2n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecBow_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecTfIDF_1n': DistanceRecommender(vectorizer=TfidfVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecTfIDF_1n_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecTfIDF_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecTfIDF_1n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecTfIDF_2n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t),\n    'CosRecTfIDF_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v2_t)\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name + '_filter_v2_t'])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.sort_values('MRR', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Результаты улучшились Теперь с транслитизацие и разбиением на пары символов:","metadata":{}},{"cell_type":"code","source":"def string_filter_v3_t(string,\n                  ru_stop=ru_stop,\n                  eng_stop=eng_stop,\n                  ru_stemmer=ru_stemmer,\n                  eng_stemmer=eng_stemmer,\n                  ngram_len=2,\n                  transliterator=translit):\n    \n    string = string.lower() \n    string = replace_values_kg(replace_values_l(string))\n    string = re.sub(r'[^a-zo0-9а-я\\s:]', '', string)\n    string = re.sub(r'(?<=[а-я])(?=[a-z])|(?<=[a-z])(?=[а-я])', ' ', string)\n    string = re.sub(r'(?<=[а-яa-z])(?=\\d)|(?<=\\d)(?=[а-яa-z])', ' ', string)\n    string = ' '.join([eng_stemmer.stem(ru_stemmer.stem(word)) for word in string.split() if word not in ru_stop+eng_stop])\n    string = ' '.join([' '.join(generate_ngrams(word, ngram_len)) if (len(word) >= ngram_len and not word.isdigit()) else word for word in string.split()])\n    string = transliterator(string, 'ru')\n    return string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n    'CosRecBow_1n': DistanceRecommender(vectorizer=CountVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecBow_1n_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecBow_2n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecBow_1n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecBow_2n_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecBow_3n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecBow_1n_4n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(1,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecBow_2n_4n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(2,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecBow_3n_4n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(3,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecBow_4n': DistanceRecommender(vectorizer=CountVectorizer(ngram_range=(4,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_1n': DistanceRecommender(vectorizer=TfidfVectorizer(), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_1n_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_2n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,2)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_1n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_2n_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_3n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(3,3)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_1n_4n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(1,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_2n_4n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(2,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_3n_4n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(3,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n    'CosRecTfIDF_4n': DistanceRecommender(vectorizer=TfidfVectorizer(ngram_range=(4,4)), simularity_func=cosine_similarity, text_prep_func=string_filter_v3_t),\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name + '_filter_v3_t'])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.sort_values('MRR', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Транслитирация немного улучшает результаты, а разбиение токенов на пары символов - наоборот. Похоже что мы выжали максимум из мешков слов, а значит двигаемся к плотным векторным представлениям:","metadata":{}},{"cell_type":"code","source":"def string_filter_emb(string):\n    \n    string = string.lower() \n    string = replace_values_kg(replace_values_l(string))\n    string = re.sub(r'[^a-zo0-9а-я\\s:]', ' ', string)\n    string = re.sub(r'(?<=[а-я])(?=[a-z])|(?<=[a-z])(?=[а-я])', ' ', string)\n    string = re.sub(r'(?<=[а-яa-z])(?=\\d)|(?<=\\d)(?=[а-яa-z])', ' ', string)\n    return string\n\nstring = dealerprice['product_name'][0]\nprint(string)\nprint(string_filter_emb(string))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\nbert = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Оборачиваем sbert, чтобы работал как обычный векторайзер sklearn (чтобы корректно встал в DistanceRecommender). Далее аналогичные действия для других моделей не комментируются ^^","metadata":{}},{"cell_type":"code","source":"class SbertVectorizer():\n    \n    def __init__(self,\n                 tokenizer=tokenizer,\n                 model=bert):\n        \n        self.tokenizer = tokenizer\n        self.model = model\n    \n    def fit(self, X=None):        \n        pass\n    \n\n    \n    def transform(self, corpus):\n        encoded_input = tokenizer(corpus, padding=True, truncation=True, max_length=24, return_tensors='pt')\n        \n        with torch.no_grad():\n            model_output = self.model(**encoded_input) \n        \n        token_embeddings = model_output[0]\n        input_mask_expanded = encoded_input['attention_mask'].unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        \n        sentence_embeddings = sum_embeddings / sum_mask\n        \n        return sentence_embeddings.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {'CosEmb_sbert': DistanceRecommender(vectorizer=SbertVectorizer(tokenizer=tokenizer, model=bert), simularity_func=cosine_similarity, text_prep_func=string_filter_emb)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.loc['CosEmb_sbert', :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tokenizer, bert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Наблюдаем низкий MRR, а это означает что модель нам не подходит. Вероятно проблема в том, что sbert корректно работает только с русскими токенами, а у нас в неймингах присутствуют английские.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\nbert = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertVectorizer():\n    \n    def __init__(self,\n                 tokenizer=tokenizer,\n                 model=bert):\n        \n        self.tokenizer = tokenizer\n        self.model = model\n    \n    def fit(self, X=None):        \n        pass\n    \n\n    \n    def transform(self, text):\n        t = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            model_output = self.model(**{k: v.to(self.model.device) for k, v in t.items()})\n        embeddings = model_output.last_hidden_state[:, 0, :]\n        embeddings = torch.nn.functional.normalize(embeddings)\n        return embeddings.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {'RubertTiny2_Cos': DistanceRecommender(vectorizer=BertVectorizer(tokenizer=tokenizer, model=bert), simularity_func=cosine_similarity, text_prep_func=string_filter_emb,)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.loc[models.keys()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tokenizer, bert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Наконец-то более-менее конкурентноспособные результаты от bert-like модели. Посмотрим реализацию sbert_large_mt_nlu_ru:","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_mt_nlu_ru\")\nbert = AutoModel.from_pretrained(\"ai-forever/sbert_large_mt_nlu_ru\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {'CosEmb_mt_sbert': DistanceRecommender(vectorizer=SbertVectorizer(tokenizer=tokenizer, model=bert), simularity_func=cosine_similarity, text_prep_func=string_filter_emb)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.loc[models.keys()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del bert, tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No comments, двигаемся дальше. Теперь можно посмотреть берты работающие с несколькими языками, одним из лучших считаются LaBSE. Для начала глянем урезанную LaBSE, работающую только с русскими и английскими токенами:","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/LaBSE-en-ru\")\nbert = AutoModel.from_pretrained(\"cointegrated/LaBSE-en-ru\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {'CosEmb_LaBSE-en-ru': DistanceRecommender(vectorizer=BertVectorizer(tokenizer=tokenizer, model=bert), simularity_func=cosine_similarity, text_prep_func=string_filter_emb)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.loc[models.keys()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del bert, tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Метрика лучше чем у предыдущих бертов. Теперь посмотрим большую Labse. Приятно, что у неё есть обёртка SentenceTransformer.","metadata":{}},{"cell_type":"code","source":"transformer = SentenceTransformer('sentence-transformers/LaBSE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerVectorizer():\n    \n    def __init__(self,\n                 transformer=transformer):\n        \n        self.transformer = transformer\n    \n    def fit(self, X=None):        \n        pass\n    \n    def transform(self, corpus):\n        embeddings = transformer.encode(corpus) \n        return embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {'CosEmb_LaBSE': DistanceRecommender(vectorizer=TransformerVectorizer(transformer=transformer), simularity_func=cosine_similarity, text_prep_func=string_filter_emb)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.loc[models.keys()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del transformer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Продвинулись дальше, но пока не догнали мешки с n-граммами. Напоследок посмотрим state of the art векторайзер на русских текстах:","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\nvectorizer = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n\nclass InfloatVectorizer():\n    \n    def __init__(self,\n                 tokenizer=tokenizer,\n                 vectorizer=vectorizer):\n        \n        self.tokenizer = tokenizer\n        self.model = vectorizer\n    \n    def fit(self, X=None):        \n        pass\n    \n\n    \n    def transform(self, corpus):\n        batch_dict = self.tokenizer(corpus, max_length=512, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            outputs = self.model(**batch_dict)\n        last_hidden = outputs.last_hidden_state.masked_fill(~batch_dict['attention_mask'][..., None].bool(), 0.0)\n        embeddings = last_hidden.sum(dim=1) / batch_dict['attention_mask'].sum(dim=1)[..., None]\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n        return embeddings.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {'Infloat_multilingual': DistanceRecommender(vectorizer=InfloatVectorizer(tokenizer=tokenizer, vectorizer=vectorizer), simularity_func=cosine_similarity, text_prep_func=string_filter_emb)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.fit(product, 'name', 'id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    results = []\n    acc_10 = []\n    acc_5 = []\n    acc_3 = []\n    acc_1 = []\n    \n    for train_ind, test_ind in tqdm(kf.split(interactions)):\n\n        test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n        true_ids = test_interactions['product_id'].values\n\n        recommendations = model.recommend(test_interactions['product_name'])\n        \n        results += [mean_reciprocal_rank(true_ids, recommendations)]\n        acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n        acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n        acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n        acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \n    results_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_table.sort_values('MRR', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Наконец-то удалось победить мешок с n-граммами. Хочется отметить, что помимо показанных выше bert-like моделей командой были рассмотрены и отвалидированы несколько других. К сожалению они показали совсем плохие результаты, и тут мы их не смотрим. Ради инетереса (больше для фана) посмотрим самую популярную метрику хакатона - accuracy@k.","metadata":{}},{"cell_type":"code","source":"del tokenizer, vectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Результаты рассмотренных решений","metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:29:05.547436Z","iopub.execute_input":"2023-11-28T11:29:05.548008Z","iopub.status.idle":"2023-11-28T11:29:05.556880Z","shell.execute_reply.started":"2023-11-28T11:29:05.547965Z","shell.execute_reply":"2023-11-28T11:29:05.555064Z"}}},{"cell_type":"code","source":"results_table.sort_values('MRR', ascending=False).head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Выводы:\n\nЛучший MRR показала модель, ранжирующая по косинусному сходству эмбеддинги xlm-roberta-large модели, предобученной командой *intfloat* (https://huggingface.co/intfloat/multilingual-e5-large). Текущее решение уже показывает отличные результаты, и вполне способно стать финальным. \nНа кросс-валидации модель демонстрирует **0.811** **MRR@10**. Это означает, что **средний ранг** релевантного айтема в списке рекомендаций модели приблизительно \nравен **1.23** (чаще всего на первом месте, редко на втором, редко-редко на других). Таким образом разметчик сможет практически моментально находить релевантный айтем заказчика.","metadata":{}},{"cell_type":"markdown","source":"#### Отдаём backend команде:","metadata":{}},{"cell_type":"markdown","source":"Набор функций и методов можно найти в `main.py`. При первой инициализации в текущей директории подгружаются все файлы, необходимые для её работы. Предусмотрена возможность переобучить на новой базе данных заказчика. Для этого нужно вызвать метод `fit` модели на датасете содержащем информацию с ключами и неймингами товаров заказчика - все нужные директории и файлы обновяются автоматически.","metadata":{"execution":{"iopub.status.busy":"2023-11-29T15:45:56.080150Z","iopub.execute_input":"2023-11-29T15:45:56.080677Z","iopub.status.idle":"2023-11-29T15:45:56.094631Z","shell.execute_reply.started":"2023-11-29T15:45:56.080635Z","shell.execute_reply":"2023-11-29T15:45:56.092898Z"}}},{"cell_type":"markdown","source":"### Улучшение результата после дедлайна","metadata":{}},{"cell_type":"markdown","source":"**Почему отказались от идеи реранжирования классификатором**\n\nПопытка реранжировать результаты работы модели первого этапа классификатором увенчалась большим провалом. Мы проверили очень много разных способов обучать классификатор catboost на аутпутах модели первого этапа, и лучший результат которого нам удалось достичь это 0.4 MRR. Насколько мы видим основная проблема подхода в малом количестве положительных примеров матчинга. Это скажем мягко говоря sad. К слову, так же пробовали обогащать вектора мешков с n-граммами разными фичами и так же ранжировать по расстоянию. Это оказался менее неудачный опыт, но всё же неудачный. Из всего заключаем, что первое решение теперь финальное.","metadata":{}},{"cell_type":"markdown","source":"#### Попробуем улучшить решение ","metadata":{}},{"cell_type":"code","source":"def names_join_ozon(x):\n    total = []\n    if type(x['name']) == str:\n        total += [x['name'].strip()]\n    if type(x['ozon_name']) == str:\n        total += [x['ozon_name'].strip()]\n    return ' '.join(total)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:34.538504Z","iopub.execute_input":"2023-12-12T00:29:34.539132Z","iopub.status.idle":"2023-12-12T00:29:34.546006Z","shell.execute_reply.started":"2023-12-12T00:29:34.539083Z","shell.execute_reply":"2023-12-12T00:29:34.544716Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def remove_dots_except_between_numbers(string):\n    new_string = ''\n    for i, char in enumerate(string):\n        if char in [',', '.']:\n            if (string[i-1].isdigit() and\n                i + 1 < len(string) and\n                string[i+1].isdigit()):\n                new_string += char\n            else:\n                new_string += ' '\n                pass\n\n        else:\n            new_string += char\n        previous_char = char\n\n    return new_string","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:36.036063Z","iopub.execute_input":"2023-12-12T00:29:36.036479Z","iopub.status.idle":"2023-12-12T00:29:36.043595Z","shell.execute_reply.started":"2023-12-12T00:29:36.036448Z","shell.execute_reply":"2023-12-12T00:29:36.042495Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def replace_values(string):\n    res = []\n    \n    string = remove_dots_except_between_numbers(string)\n    \n    splitted = string.split()\n    \n    for i, t in enumerate(splitted):\n        if 'л' in t:\n            value = t.replace('л', '')\n            if value.replace(',', '.').replace('.', '').isdigit():\n                value = float(value.replace(',', '.')) * 1000\n                t = f'{int(value)} мл'\n                res += [t]\n                continue\n\n            elif splitted[i - 1].replace(',', '.').replace('.', '').isdigit() and not 'мл' in t:\n                value = float(splitted[i - 1].replace(',', '.')) * 1000\n                t = f'{int(value)} мл'\n                res = res[:-1]\n                res += [t]\n                continue\n            else:\n                pass\n        if 'кг' in t:\n            value = t.replace('кг', '')\n            if value.replace(',', '.').replace('.', '').isdigit():\n                value = float(value.replace(',', '.')) * 1000\n                t = f'{int(value)} г'\n                res += [t]\n                continue\n\n            elif splitted[i - 1].replace(',', '.').replace('.', '').isdigit():\n                value = float(splitted[i - 1].replace(',', '.')) * 1000\n                t = f'{int(value)} г'\n                res = res[:-1]\n                res += [t]\n                continue\n            else:\n                pass\n        \n        res += [t]\n    return ' '.join(res)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:37.710210Z","iopub.execute_input":"2023-12-12T00:29:37.711092Z","iopub.status.idle":"2023-12-12T00:29:37.722015Z","shell.execute_reply.started":"2023-12-12T00:29:37.711048Z","shell.execute_reply":"2023-12-12T00:29:37.720546Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        return super(NumpyEncoder, self).default(obj)\n\ndef string_filter_emb(string):\n    \n    string = string.lower() \n    string = re.sub(r'\\d+-\\d+', '', string)\n    string = re.sub(r'\\d+::\\d+', '', string)\n    string = re.sub(r'\\d+:\\d+', '', string)\n    string = replace_values(string)\n    string = re.sub(r'[^a-zo0-9а-я\\s:]', ' ', string)\n    string = re.sub(r'(?<=[а-я])(?=[a-z])|(?<=[a-z])(?=[а-я])', ' ', string)\n    string = re.sub(r'(?<=[а-яa-z])(?=\\d)|(?<=\\d)(?=[а-яa-z])', ' ', string)\n    \n    string = string.replace(' 0 ', ' ')\n    string = ' '.join([w for w in string.split()])\n    return string","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:39.533831Z","iopub.execute_input":"2023-12-12T00:29:39.534300Z","iopub.status.idle":"2023-12-12T00:29:39.543287Z","shell.execute_reply.started":"2023-12-12T00:29:39.534262Z","shell.execute_reply":"2023-12-12T00:29:39.541775Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def check_vectorizer_files(dirs=['./tokenizer/', './vectorizer/']):\n\n    if not os.path.exists(dirs[0]):\n        os.makedirs(dirs[0])\n        tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n        tokenizer.save_pretrained(dirs[0])\n        \n        \n    if not os.path.exists(dirs[1]):\n        os.makedirs(dirs[1])\n        vectorizer = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n        vectorizer.save_pretrained(dirs[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:41.235254Z","iopub.execute_input":"2023-12-12T00:29:41.235662Z","iopub.status.idle":"2023-12-12T00:29:41.242379Z","shell.execute_reply.started":"2023-12-12T00:29:41.235632Z","shell.execute_reply":"2023-12-12T00:29:41.241075Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class InfloatVectorizer():\n    def __init__(self,\n                 toc_path='./tokenizer/',\n                 vec_path='./vectorizer/'):\n        \n        check_vectorizer_files(dirs=[toc_path, vec_path])\n\n        self.tokenizer = AutoTokenizer.from_pretrained(toc_path)\n        self.model = AutoModel.from_pretrained(vec_path)\n\n    def fit(self, X=None):\n        pass\n\n    def transform(self, corpus):\n        batch_dict = self.tokenizer(\n            corpus,\n            max_length=512,\n            padding=True,\n            truncation=True,\n            return_tensors='pt'\n        )\n        with torch.no_grad():\n            outputs = self.model(**batch_dict)\n        last_hidden = outputs.last_hidden_state.masked_fill(\n            ~batch_dict['attention_mask'][..., None].bool(), 0.0\n        )\n        embeddings = (last_hidden.sum(dim=1)\n                      / batch_dict['attention_mask'].sum(dim=1)[..., None])\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n        return embeddings.numpy()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:42.677545Z","iopub.execute_input":"2023-12-12T00:29:42.677971Z","iopub.status.idle":"2023-12-12T00:29:42.688062Z","shell.execute_reply.started":"2023-12-12T00:29:42.677938Z","shell.execute_reply":"2023-12-12T00:29:42.686760Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class DistanceRecommender():\n    def __init__(self,\n                 vectorizer,\n                 simularity_func,\n                 text_prep_func=string_filter_emb):\n        self.vectorizer = vectorizer\n        self.simularity_counter = simularity_func\n        self.preprocessing = text_prep_func\n\n    def fit(self,\n            product_corpus,\n            name_column,\n            id_column,\n            save_to_dir=False):\n        preprocessed_corpus = (\n            product_corpus[name_column].apply(\n                self.preprocessing\n            ).values.tolist()\n        )\n        self.vectorizer.fit(preprocessed_corpus)\n        self.product_matrix = self.vectorizer.transform(preprocessed_corpus)\n        self.product_index_to_id = {str(i): product_corpus.loc[i, id_column] for i in range(len(product_corpus))}\n        if save_to_dir:\n            \n            if not os.path.exists('./model_files'):\n                os.makedirs('./model_files')\n            \n            np.save('./model_files/product_matrix.npy', self.product_matrix)\n\n            with open('./model_files/product_index_to_id.json', 'w') as file:\n                json.dump(self.product_index_to_id, file, cls=NumpyEncoder)\n\n    def from_pretrained(\n        self,\n        product_matrix_path='./model_files/product_matrix.npy',\n        product_index_to_id_dict_path='./model_files/product_index_to_id.json'\n    ):\n        self.product_matrix = np.load(product_matrix_path)\n\n        with open(product_index_to_id_dict_path, 'rb') as file:\n            self.product_index_to_id = json.load(file)\n\n    def recommend(self,\n                  dealer_corpus: list[dict]):\n        dealer_corpus = pd.Series(dealer_corpus)\n\n        dealer_corpus = dealer_corpus.apply(\n            self.preprocessing\n        ).values.tolist()\n        vectors = self.vectorizer.transform(dealer_corpus)\n        sims = self.simularity_counter(vectors, self.product_matrix)\n\n        result = []\n        for vec in sims:\n            result += [[self.product_index_to_id[str(index)] for index in vec.argsort()[::-1]]]\n        return np.array(result)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:44.943375Z","iopub.execute_input":"2023-12-12T00:29:44.944133Z","iopub.status.idle":"2023-12-12T00:29:44.956391Z","shell.execute_reply.started":"2023-12-12T00:29:44.944083Z","shell.execute_reply":"2023-12-12T00:29:44.955349Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"product['names_with_ozon'] = product.apply(names_join_ozon, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:51.402548Z","iopub.execute_input":"2023-12-12T00:29:51.403011Z","iopub.status.idle":"2023-12-12T00:29:51.422546Z","shell.execute_reply.started":"2023-12-12T00:29:51.402973Z","shell.execute_reply":"2023-12-12T00:29:51.421108Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Все нужные для работы модели файлы автоматически скачиваются и сохраняются в текущей директории. После обучения там же сохраняются векторы товаров Prosept и другие вспомогательные файлы (`model_files`). При последующем использовании можно использовать метод `model.from_pretrained`.","metadata":{}},{"cell_type":"code","source":"model = DistanceRecommender(InfloatVectorizer(), cosine_similarity, string_filter_emb)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:30:13.178677Z","iopub.execute_input":"2023-12-12T00:30:13.179087Z","iopub.status.idle":"2023-12-12T00:30:22.011338Z","shell.execute_reply.started":"2023-12-12T00:30:13.179056Z","shell.execute_reply":"2023-12-12T00:30:22.009333Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Чтобы обучить модель на новой таблице с информацией о товарах prosept, передаём методу fit таблицу и наименования колонок с неймингами и внутренними id товаров. Чтобы файлы модели сохранились, указываем `save_to_dir=True`.","metadata":{}},{"cell_type":"code","source":"model.fit(product, 'names_with_ozon', 'id', save_to_dir=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь воспользуемся методом `model.from_pretrained`:","metadata":{}},{"cell_type":"code","source":"model.from_pretrained()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:30:35.833011Z","iopub.execute_input":"2023-12-12T00:30:35.833419Z","iopub.status.idle":"2023-12-12T00:30:35.842569Z","shell.execute_reply.started":"2023-12-12T00:30:35.833389Z","shell.execute_reply":"2023-12-12T00:30:35.841386Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Отвалидируем наше финальное решение. ","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=3)\nresults_table = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:31:26.678314Z","iopub.execute_input":"2023-12-12T00:31:26.678756Z","iopub.status.idle":"2023-12-12T00:31:26.685160Z","shell.execute_reply.started":"2023-12-12T00:31:26.678719Z","shell.execute_reply":"2023-12-12T00:31:26.683961Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"name = 'final_model'\nresults = []\nacc_10 = []\nacc_5 = []\nacc_3 = []\nacc_1 = []\n    \nfor train_ind, test_ind in tqdm(kf.split(interactions)):\n\n    test_interactions = interactions.loc[test_ind, :].merge(dealerprice,\n                                                                left_on=['key', 'dealer_id'],\n                                                                right_on=['product_key', 'dealer_id'],\n                                                                how='inner')\n\n    true_ids = test_interactions['product_id'].values\n\n    recommendations = model.recommend(test_interactions['product_name'])\n        \n    results += [mean_reciprocal_rank(true_ids, recommendations)]\n    acc_10 += [accuracy_k(true_ids, recommendations, 10)]\n    acc_5 += [accuracy_k(true_ids, recommendations, 5)]\n    acc_3 += [accuracy_k(true_ids, recommendations, 3)]\n    acc_1 += [accuracy_k(true_ids, recommendations, 1)]\n    \nresults_table = pd.concat([results_table, pd.DataFrame({'MRR': np.mean(results),\n                                                            'MRR_std': np.std(results),\n                                                            'acc_10': np.mean(acc_10),\n                                                            'acc_5': np.mean(acc_5),\n                                                            'acc_3': np.mean(acc_3),\n                                                            'acc_1': np.mean(acc_1)},\n                                                           index=[name])])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:33:24.511018Z","iopub.execute_input":"2023-12-12T00:33:24.511483Z","iopub.status.idle":"2023-12-12T00:38:33.454015Z","shell.execute_reply.started":"2023-12-12T00:33:24.511444Z","shell.execute_reply":"2023-12-12T00:38:33.452629Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"3it [05:08, 102.98s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"results_table","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:38:33.456180Z","iopub.execute_input":"2023-12-12T00:38:33.456637Z","iopub.status.idle":"2023-12-12T00:38:33.469206Z","shell.execute_reply.started":"2023-12-12T00:38:33.456595Z","shell.execute_reply":"2023-12-12T00:38:33.468344Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"                  MRR   MRR_std    acc_10     acc_5     acc_3     acc_1\nfinal_model  0.856305  0.030148  0.971057  0.946054  0.917287  0.788175","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MRR</th>\n      <th>MRR_std</th>\n      <th>acc_10</th>\n      <th>acc_5</th>\n      <th>acc_3</th>\n      <th>acc_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>final_model</th>\n      <td>0.856305</td>\n      <td>0.030148</td>\n      <td>0.971057</td>\n      <td>0.946054</td>\n      <td>0.917287</td>\n      <td>0.788175</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Результат","metadata":{}},{"cell_type":"markdown","source":"В ходе работ нам удалось достичь **0.856 MRR**, это означает что среднее значение позиции правильного ответа приблизительно равно **1.17**. Вероятность того, что правильный ответ окажется **на первом месте** рекомендаций приблизительно равно **79%**, **91%** что войдёт в **топ 3**, **97%** - в **топ 10**. ","metadata":{}}]}